import asyncio
import logging
from dotenv import load_dotenv
import os
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from transformers import AutoModelForCausalLM, AutoTokenizer
from database import init_db, get_history, update_history, clear_history
import torch
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()
BOT_TOKEN = os.getenv("BOT_TOKEN")
ADMIN_CHAT_ID = int(os.getenv("ADMIN_CHAT_ID"))
HISTORY_LIMIT = int(os.getenv("HISTORY_LIMIT", 10))

if not BOT_TOKEN:
    raise ValueError("–¢–æ–∫–µ–Ω –±–æ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env!")

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    use_flash_attention_2=False
)

device_info = f"–ú–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}"
logger.info(device_info)
cuda_available = torch.cuda.is_available()
logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {cuda_available}")

if cuda_available:
    cuda_device_count = torch.cuda.device_count()
    cuda_device_name = torch.cuda.get_device_name(0) if cuda_device_count > 0 else "–ù–µ—Ç"
    logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {cuda_device_count}")
    logger.info(f"–ù–∞–∑–≤–∞–Ω–∏–µ GPU: {cuda_device_name}")
    logger.info(f"–¢–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏: {torch.cuda.memory_allocated() / 1024**2:.2f} –ú–ë")
    logger.info(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.2f} –ú–ë")

bot = Bot(token=BOT_TOKEN)
dp = Dispatcher()

executor = ThreadPoolExecutor(max_workers=1)

@dp.message(Command("start"))
async def start(message: types.Message):
    await message.answer(
        "–Ø –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ DeepSeek-R1. "
        "–ß—Ç–æ–±—ã —Å–±—Ä–æ—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ - /new")

@dp.message(Command("new"))
async def new_chat(message: types.Message):
    await clear_history(message.from_user.id)
    await message.answer("‚öîÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ —Å–±—Ä–æ—à–µ–Ω!‚öîÔ∏è")

@dp.message(F.text)
async def handle_message(message: types.Message):
    user_id = message.from_user.id
    user_name = message.from_user.username
    chat_id = message.chat.id
    user_text = message.text
    logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} {user_name}: {user_text[:50]}...")
    try:
        await bot.send_chat_action(chat_id=chat_id, action="typing")
        loop = asyncio.get_event_loop()
        typing_task = asyncio.create_task(keep_typing(chat_id))
        logger.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}...")
        assistant_reply, context = await loop.run_in_executor(executor, sync_generate_response, user_id, user_text)
        logger.info(f"–û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        typing_task.cancel()
        await message.answer(assistant_reply)
        logger.info(f"–û—Ç–≤–µ—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")
        await bot.send_message(
            ADMIN_CHAT_ID,
            f"üì® –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} {user_name}:\n\n"
            f"<b>–ü—Ä–æ–º–ø—Ç, –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –≤ –º–æ–¥–µ–ª—å:</b>\n"
            f"<code>{context}</code>\n\n"
            f"<b>–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:</b>\n"
            f"<code>{assistant_reply}</code>",
            parse_mode="HTML"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {str(e)}", exc_info=True)
        await message.answer(f"–û—à–∏–±–∫–∞: {str(e)}")

async def keep_typing(chat_id):
    try:
        typing_count = 0
        while True:
            await bot.send_chat_action(chat_id=chat_id, action="typing")
            typing_count += 1
            await asyncio.sleep(3)
    except asyncio.CancelledError:
        pass
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ keep_typing: {str(e)}", exc_info=True)

def sync_generate_response(user_id, message_text):
    import asyncio
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        history = loop.run_until_complete(get_history(user_id))
        context = '\n'.join(history + [f"User: {message_text}"]) + "\nAssistant:"
        inputs = tokenizer(context, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_reply = response[len(context):].strip().split('\n')[0]
        loop.run_until_complete(update_history(user_id, message_text, assistant_reply, HISTORY_LIMIT))
        loop.close()
        return assistant_reply, context
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ generate_response: {str(e)}", exc_info=True)
        raise

async def main():
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    await init_db()
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    await dp.start_polling(bot)

if __name__ == "__main__":
    try:
        logger.info("–ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
        asyncio.run(main())
    except (KeyboardInterrupt, SystemExit):
        logger.info("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    except Exception as e:
        logger.critical(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}", exc_info=True)
